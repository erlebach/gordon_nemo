name: MultiBlockRegressorModel
do_training: true

# Architecture parameters using a different namespace
arch:
  input_dim: 1
  hidden_dim: 16
  output_dim: 1
  num_blocks: 2
  num_layers_per_block: 2
  activation: "tanh"

# Model IO parameters
io:
  nemo_path: multiblock_base.nemo  # path to save .nemo file
  restore_path: null  # path to restore model from

# Dataset configurations
train_ds:
  file_path: "data/base/sine_train.npz"
  batch_size: 32
  num_workers: 0
  pin_memory: false
  
validation_ds:
  file_path: "data/base/sine_val.npz"
  batch_size: 32
  num_workers: 0
  pin_memory: false
  
test_ds:
  file_path: "data/base/sine_test.npz"
  batch_size: 32
  num_workers: 0
  pin_memory: false
  
# Optimizer configuration
optim:
  name: adam
  lr: 0.01
  weight_decay: 0.0
  
  sched:
    name: CosineAnnealing
    T_max: 100
    min_lr: 0.0001

# Adapter configuration for fine-tuning
adapter:
  name: "lora_adapter"
  dim: 8
  alpha: 32
  dropout: 0.1
  target_blocks: [0, 1]  # Which blocks to apply adapters to (null for all blocks)

trainer:
  devices: 1
  num_nodes: 1
  max_epochs: 50
  precision: 32
  accelerator: auto
  strategy: auto
  enable_checkpointing: false
  logger: false
  log_every_n_steps: 10
  check_val_every_n_epoch: 1
  benchmark: false

exp_manager:
  exp_dir: null  # default exp_dir is ./nemo_experiments
  name: ${name}
  create_tensorboard_logger: true
  create_checkpoint_callback: true
  checkpoint_callback_params:
    monitor: val_loss
    mode: min
    save_top_k: 3
    always_save_nemo: false
  resume_if_exists: true
  resume_ignore_no_checkpoint: true

hydra:
  run:
    dir: .
  job_logging:
    root:
      handlers: null
