{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Goals\n",
    "\n",
    "## Optimizing Foundation Models with Supervised Fine-Tuning (SFT)\n",
    "\n",
    "Often, we want to adapt or customize foundation models to be more performant on our specific task. Fine-tuning refers to how we can modify the weights of a pre-trained foundation model with additional custom data. Supervised Fine-Tuning (SFT) refers to unfreezing all the weights and layers in our model and training on a newly labeled set of examples. We can fine-tune to incorporate new, domain-specific knowledge, or teach the foundation model what type of response to provide. One specific type of SFT is also referred to as “instruction tuning” where we use SFT to teach a model to follow instructions better. In this tutorial, will demonstrate how to perform SFT with Llama3-8b using NeMo 2.0.\n",
    "\n",
    "NeMo 2.0 introduces Python-based configurations, PyTorch Lightning’s modular abstractions, and NeMo-Run for scaling experiments across multiple GPUs. In this notebook, we will use NeMo-Run to streamline the configuration and execution of our experiments.\n",
    "\n",
    "## Data\n",
    "Databricks-dolly-15k is an open-source dataset created by the collaborative efforts of Databricks employees. It consists of high-quality, human-generated prompt/response pairs specifically designed for instruction tuning LLMs. These pairs cover a diverse range of behaviors, from brainstorming and content generation to information extraction and summarization. \n",
    "\n",
    "For more information, refer to [databricks-dolly-15k | Hugging Face](https://huggingface.co/datasets/databricks/databricks-dolly-15k)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1. Import the Hugging Face Checkpoint\n",
    "We use the `llm.import_ckpt` API to download the specified model using the \"hf://<huggingface_model_id>\" URL format. It will then convert the model into NeMo 2.0 format. For all model supported in NeMo 2.0, refer to [Large Language Models](https://docs.nvidia.com/nemo-framework/user-guide/24.09/llms/index.html#large-language-models) section of NeMo Framework User Guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.10.16 environment at: /Users/erlebach/src/2025/gordon_nemo/.venv\u001b[0m\n",
      "Package                   Version        Editable project location\n",
      "------------------------- -------------- ------------------------------------\n",
      "absl-py                   2.2.2\n",
      "aiohappyeyeballs          2.6.1\n",
      "aiohttp                   3.11.18\n",
      "aiosignal                 1.3.2\n",
      "aniso8601                 10.0.1\n",
      "annotated-types           0.7.0\n",
      "antlr4-python3-runtime    4.9.3\n",
      "anyio                     4.9.0\n",
      "appnope                   0.1.4\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "asciitree                 0.3.3\n",
      "asttokens                 3.0.0\n",
      "async-lru                 2.0.5\n",
      "async-timeout             5.0.1\n",
      "attrs                     25.3.0\n",
      "audioread                 3.0.1\n",
      "babel                     2.17.0\n",
      "bcrypt                    4.3.0\n",
      "beautifulsoup4            4.13.4\n",
      "bleach                    6.2.0\n",
      "blinker                   1.9.0\n",
      "boto3                     1.38.18\n",
      "botocore                  1.38.18\n",
      "braceexpand               0.1.7\n",
      "catalogue                 2.0.10\n",
      "certifi                   2025.4.26\n",
      "cffi                      1.17.1\n",
      "charset-normalizer        3.4.2\n",
      "click                     8.2.0\n",
      "cloudpickle               3.1.1\n",
      "colorama                  0.4.6\n",
      "comm                      0.2.2\n",
      "contourpy                 1.3.2\n",
      "coverage                  7.8.0\n",
      "cryptography              42.0.8\n",
      "cycler                    0.12.1\n",
      "cytoolz                   1.0.1\n",
      "datasets                  3.6.0\n",
      "debugpy                   1.8.14\n",
      "decorator                 5.2.1\n",
      "defusedxml                0.7.1\n",
      "deprecated                1.2.18\n",
      "dill                      0.3.8\n",
      "docker                    7.1.0\n",
      "docker-pycreds            0.4.0\n",
      "docopt                    0.6.2\n",
      "docstring-parser          0.16\n",
      "editdistance              0.8.1\n",
      "einops                    0.8.1\n",
      "exceptiongroup            1.3.0\n",
      "executing                 2.2.0\n",
      "fabric                    3.2.2\n",
      "fasteners                 0.19\n",
      "fastjsonschema            2.21.1\n",
      "ffmpeg                    1.4\n",
      "fiddle                    0.3.0\n",
      "filelock                  3.18.0\n",
      "flask                     3.1.1\n",
      "flask-restful             0.3.10\n",
      "fonttools                 4.58.0\n",
      "fqdn                      1.5.1\n",
      "frozenlist                1.6.0\n",
      "fsspec                    2025.3.0\n",
      "gitdb                     4.0.12\n",
      "gitpython                 3.1.44\n",
      "gordon-nemo               0.1.0          /Users/erlebach/src/2025/gordon_nemo\n",
      "graphviz                  0.20.3\n",
      "grpcio                    1.71.0\n",
      "h11                       0.16.0\n",
      "h5py                      3.13.0\n",
      "httpcore                  1.0.9\n",
      "httpx                     0.28.1\n",
      "huggingface-hub           0.31.2\n",
      "hydra-core                1.3.2\n",
      "idna                      3.10\n",
      "ijson                     3.4.0\n",
      "importlib-metadata        8.7.0\n",
      "iniconfig                 2.1.0\n",
      "inquirerpy                0.3.4\n",
      "intervaltree              3.1.0\n",
      "invoke                    2.2.0\n",
      "ipykernel                 6.29.5\n",
      "ipython                   8.36.0\n",
      "isoduration               20.11.0\n",
      "itsdangerous              2.2.0\n",
      "jaxtyping                 0.3.2\n",
      "jedi                      0.19.2\n",
      "jieba                     0.42.1\n",
      "jinja2                    3.1.6\n",
      "jiwer                     3.1.0\n",
      "jmespath                  1.0.1\n",
      "joblib                    1.5.0\n",
      "json5                     0.12.0\n",
      "jsonpointer               3.0.0\n",
      "jsonschema                4.23.0\n",
      "jsonschema-specifications 2025.4.1\n",
      "jupyter-client            8.6.3\n",
      "jupyter-core              5.7.2\n",
      "jupyter-events            0.12.0\n",
      "jupyter-lsp               2.2.5\n",
      "jupyter-server            2.16.0\n",
      "jupyter-server-terminals  0.5.3\n",
      "jupyterlab                4.4.2\n",
      "jupyterlab-pygments       0.3.0\n",
      "jupyterlab-server         2.27.3\n",
      "kiwisolver                1.4.8\n",
      "lazy-loader               0.4\n",
      "lhotse                    1.30.3\n",
      "libcst                    1.7.0\n",
      "librosa                   0.11.0\n",
      "lightning                 2.4.0\n",
      "lightning-utilities       0.14.3\n",
      "lilcom                    1.8.1\n",
      "llvmlite                  0.44.0\n",
      "lxml                      5.4.0\n",
      "markdown                  3.8\n",
      "markdown-it-py            3.0.0\n",
      "markupsafe                3.0.2\n",
      "matplotlib                3.10.3\n",
      "matplotlib-inline         0.1.7\n",
      "mdurl                     0.1.2\n",
      "megatron-core             0.12.0\n",
      "mistune                   3.1.3\n",
      "ml-dtypes                 0.5.1\n",
      "mpmath                    1.3.0\n",
      "msgpack                   1.1.0\n",
      "multidict                 6.4.3\n",
      "multiprocess              0.70.16\n",
      "mypy-extensions           1.1.0\n",
      "nbclient                  0.10.2\n",
      "nbconvert                 7.16.6\n",
      "nbformat                  5.10.4\n",
      "nem                       0.1\n",
      "nemo-run                  0.4.0\n",
      "nemo-toolkit              2.1.0\n",
      "nest-asyncio              1.6.0\n",
      "networkx                  3.4.2\n",
      "nltk                      3.9.1\n",
      "notebook-shim             0.2.4\n",
      "numba                     0.61.2\n",
      "numcodecs                 0.13.1\n",
      "numpy                     2.2.6\n",
      "omegaconf                 2.3.0\n",
      "onnx                      1.18.0\n",
      "opencc                    1.1.9\n",
      "overrides                 7.7.0\n",
      "packaging                 24.2\n",
      "pandas                    2.2.3\n",
      "pandocfilters             1.5.1\n",
      "pangu                     4.0.6.1\n",
      "paramiko                  3.5.1\n",
      "parso                     0.8.4\n",
      "pexpect                   4.9.0\n",
      "pfzy                      0.3.4\n",
      "pillow                    11.2.1\n",
      "platformdirs              4.3.8\n",
      "pluggy                    1.6.0\n",
      "pooch                     1.8.2\n",
      "portalocker               3.1.1\n",
      "prometheus-client         0.22.0\n",
      "prompt-toolkit            3.0.51\n",
      "propcache                 0.3.1\n",
      "protobuf                  6.31.0\n",
      "psutil                    7.0.0\n",
      "ptyprocess                0.7.0\n",
      "pure-eval                 0.2.3\n",
      "pyannote-core             5.0.0\n",
      "pyannote-database         5.1.3\n",
      "pyannote-metrics          3.2.1\n",
      "pyarrow                   20.0.0\n",
      "pycparser                 2.22\n",
      "pydantic                  2.11.4\n",
      "pydantic-core             2.33.2\n",
      "pygments                  2.19.1\n",
      "pynacl                    1.5.0\n",
      "pyparsing                 3.2.3\n",
      "pyre-extensions           0.0.32\n",
      "pytest                    8.3.5\n",
      "pytest-cov                6.1.1\n",
      "pytest-mock               3.14.0\n",
      "pytest-random-order       1.1.1\n",
      "python-dateutil           2.9.0.post0\n",
      "python-dotenv             1.1.0\n",
      "python-json-logger        3.3.0\n",
      "pytorch-lightning         2.5.1.post0\n",
      "pytz                      2025.2\n",
      "pyyaml                    6.0.2\n",
      "pyzmq                     26.4.0\n",
      "rapidfuzz                 3.13.0\n",
      "referencing               0.36.2\n",
      "regex                     2024.11.6\n",
      "requests                  2.32.3\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rich                      14.0.0\n",
      "rouge-score               0.1.2\n",
      "rpds-py                   0.25.0\n",
      "ruamel-yaml               0.18.10\n",
      "ruamel-yaml-clib          0.2.12\n",
      "s3transfer                0.12.0\n",
      "sacrebleu                 2.5.1\n",
      "safetensors               0.5.3\n",
      "scikit-learn              1.6.1\n",
      "scipy                     1.15.3\n",
      "seaborn                   0.13.2\n",
      "send2trash                1.8.3\n",
      "sentencepiece             0.2.0\n",
      "sentry-sdk                2.28.0\n",
      "setproctitle              1.3.6\n",
      "setuptools                80.7.1\n",
      "shellingham               1.5.4\n",
      "six                       1.17.0\n",
      "smmap                     5.0.2\n",
      "sniffio                   1.3.1\n",
      "sortedcontainers          2.4.0\n",
      "soundfile                 0.13.1\n",
      "soupsieve                 2.7\n",
      "sox                       1.5.0\n",
      "soxr                      0.5.0.post1\n",
      "stack-data                0.6.3\n",
      "sympy                     1.14.0\n",
      "tabulate                  0.9.0\n",
      "tensorboard               2.19.0\n",
      "tensorboard-data-server   0.7.2\n",
      "tensorstore               0.1.75\n",
      "terminado                 0.18.1\n",
      "text-unidecode            1.3\n",
      "threadpoolctl             3.6.0\n",
      "tiktoken                  0.9.0\n",
      "tinycss2                  1.4.0\n",
      "tokenizers                0.21.1\n",
      "tomli                     2.2.1\n",
      "toolz                     1.0.0\n",
      "torch                     2.7.0\n",
      "torchaudio                2.7.0\n",
      "torchmetrics              1.7.1\n",
      "torchx                    0.7.0\n",
      "tornado                   6.5\n",
      "tqdm                      4.67.1\n",
      "traitlets                 5.14.3\n",
      "transformers              4.51.3\n",
      "typer                     0.15.3\n",
      "types-python-dateutil     2.9.0.20250516\n",
      "typing-extensions         4.13.2\n",
      "typing-inspect            0.9.0\n",
      "typing-inspection         0.4.0\n",
      "tzdata                    2025.2\n",
      "uri-template              1.3.0\n",
      "urllib3                   1.26.20\n",
      "wadler-lindig             0.1.6\n",
      "wandb                     0.19.11\n",
      "wcwidth                   0.2.13\n",
      "webcolors                 24.11.1\n",
      "webdataset                0.2.111\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.8.0\n",
      "werkzeug                  3.1.3\n",
      "wget                      3.2\n",
      "wrapt                     1.17.2\n",
      "xxhash                    3.5.0\n",
      "yarl                      1.20.0\n",
      "zarr                      2.18.3\n",
      "zipp                      3.21.0\n"
     ]
    }
   ],
   "source": [
    "!uv pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import megatron.core.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRANSFORMERS_CACHE='/Users/erlebach/.cache/huggingface/hub'\n",
      "PYTORCH_TRANSFORMERS_CACHE='/Users/erlebach/.cache/huggingface/hub'\n",
      "HF_MODULES_CACHE='/Users/erlebach/.cache/huggingface/modules'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0529 21:29:35.624000 87975 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "[NeMo W 2025-05-29 21:29:36 nemo_logging:361] /Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/megatron/core/optimizer/__init__.py:18: UserWarning: Transformer Engine and Apex are not installed. Falling back to Torch optimizers.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2025-05-29 21:29:36 nemo_logging:361] /Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/megatron/core/optimizer/optimizer.py:28: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier and multi_tensor_scale\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2025-05-29 21:29:36 nemo_logging:361] /Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/megatron/core/optimizer/clip_grads.py:29: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier, multi_tensor_l2norm, and multi_tensor_scale\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2025-05-29 21:29:39 nemo_logging:361] /Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/megatron/core/transformer/multi_token_prediction.py:60: UserWarning: Apex is not installed. Falling back to Torch Norm\n",
      "      warnings.warn('Apex is not installed. Falling back to Torch Norm')\n",
      "    \n",
      "[NeMo W 2025-05-29 21:29:39 nemo_logging:361] /Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/megatron/core/models/gpt/gpt_layer_specs.py:65: UserWarning: Apex is not installed. Falling back to Torch Norm\n",
      "      warnings.warn('Apex is not installed. Falling back to Torch Norm')\n",
      "    \n",
      "[NeMo W 2025-05-29 21:29:39 nemo_logging:361] /Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/megatron/core/models/retro/encoder_spec.py:47: UserWarning: Apex is not installed. Falling back to Torch Norm\n",
      "      warnings.warn(f'Apex is not installed. Falling back to Torch Norm')\n",
      "    \n",
      "[NeMo W 2025-05-29 21:29:39 nemo_logging:361] /Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/megatron/core/models/retro/decoder_spec.py:39: UserWarning: Apex is not installed. Falling back to Torch Norm\n",
      "      warnings.warn(f'Apex is not installed. Falling back to Torch Norm')\n",
      "    \n",
      "[NeMo W 2025-05-29 21:29:39 nemo_logging:361] /Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/megatron/core/models/T5/t5_spec.py:43: UserWarning: Apex is not installed. Falling back to Torch Norm\n",
      "      warnings.warn(f'Apex is not installed. Falling back to Torch Norm')\n",
      "    \n",
      "[NeMo W 2025-05-29 21:29:42 nemo_logging:361] /Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/megatron/core/models/bert/bert_lm_head.py:15: UserWarning: Apex is not installed. Falling back to Torch Norm\n",
      "      warnings.warn(f'Apex is not installed. Falling back to Torch Norm')\n",
      "    \n",
      "[NeMo W 2025-05-29 21:29:42 ssm:31] The package `megatron.core` was not imported in this environment which is needed for SSMs.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">─ </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Entering Experiment nemo.collections.llm.api.import_ckpt with id: nemo.collections.llm.api.import_ckpt_1748568…</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ─</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m─ \u001b[0m\u001b[1;35mEntering Experiment nemo.collections.llm.api.import_ckpt with id: nemo.collections.llm.api.import_ckpt_1748568…\u001b[0m\u001b[92m ─\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /Users/erlebach/.nemo_run/experiments/nemo.collections.llm.api.import_ckpt/nemo.collections.llm.api.import_ckpt_1748568583/nemo.collections.llm.api.import_ckpt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[21:29:43] </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">Launching job nemo.collections.llm.api.import_ckpt for experiment </span>                     <a href=\"file:///Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/nemo_run/run/experiment.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">experiment.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/nemo_run/run/experiment.py#744\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">744</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">nemo.collections.llm.api.import_ckpt</span>                                                   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[21:29:43]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;36mLaunching job nemo.collections.llm.api.import_ckpt for experiment \u001b[0m                     \u001b]8;id=206232;file:///Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/nemo_run/run/experiment.py\u001b\\\u001b[2mexperiment.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=17840;file:///Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/nemo_run/run/experiment.py#744\u001b\\\u001b[2m744\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m           \u001b[0m\u001b[1;36mnemo.collections.llm.api.import_ckpt\u001b[0m                                                   \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Log directory is: /Users/erlebach/.nemo_run/experiments/nemo.collections.llm.api.import_ckpt/nemo.collections.llm.api.import_ckpt_1748568583/nemo.collections.llm.api.import_ckpt\n",
      "Launched app: local_persistent://nemo_run/nemo.collections.llm.api.import_ckpt-m661959kz100g\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #00ff00; text-decoration-color: #00ff00\">──────────────── </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Waiting for Experiment nemo.collections.llm.api.import_ckpt_1748568583 to finish</span><span style=\"color: #00ff00; text-decoration-color: #00ff00\"> ─────────────────</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[92m──────────────── \u001b[0m\u001b[1;35mWaiting for Experiment nemo.collections.llm.api.import_ckpt_1748568583 to finish\u001b[0m\u001b[92m ─────────────────\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Experiment Status for</span> <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">nemo.collections.llm.api.import_ckpt_1748568583</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mExperiment Status for\u001b[0m \u001b[1;38;5;214mnemo.collections.llm.api.import_ckpt_1748568583\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Task 0</span>: <span style=\"color: #ffaf00; text-decoration-color: #ffaf00; font-weight: bold\">nemo.collections.llm.api.import_ckpt</span>\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Status</span>: RUNNING\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Executor</span>: LocalExecutor\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Job id</span>: nemo.collections.llm.api.import_ckpt-m661959kz100g\n",
       "- <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Local Directory</span>: /Users/erlebach/.nemo_run/experiments/nemo.collections.llm.api.import_ckpt/nemo.collections.llm.api.import_ckpt_1748568583/nemo.collections.llm.api.import_ckpt\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1;32mTask 0\u001b[0m: \u001b[1;38;5;214mnemo.collections.llm.api.import_ckpt\u001b[0m\n",
       "- \u001b[1;32mStatus\u001b[0m: RUNNING\n",
       "- \u001b[1;32mExecutor\u001b[0m: LocalExecutor\n",
       "- \u001b[1;32mJob id\u001b[0m: nemo.collections.llm.api.import_ckpt-m661959kz100g\n",
       "- \u001b[1;32mLocal Directory\u001b[0m: /Users/erlebach/.nemo_run/experiments/nemo.collections.llm.api.import_ckpt/nemo.collections.llm.api.import_ckpt_1748568583/nemo.collections.llm.api.import_ckpt\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting for job nemo.collections.llm.api.import_ckpt-m661959kz100g to finish [log=True]...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mport_ckpt/0 TRANSFORMERS_CACHE='/Users/erlebach/.cache/huggingface/hub'\n",
      "mport_ckpt/0 PYTORCH_TRANSFORMERS_CACHE='/Users/erlebach/.cache/huggingface/hub'\n",
      "mport_ckpt/0 HF_MODULES_CACHE='/Users/erlebach/.cache/huggingface/modules'\n",
      "mport_ckpt/0 W0529 21:29:47.460000 88269 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "mport_ckpt/0 [NeMo W 2025-05-29 21:29:48 nemo_logging:361] /Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/megatron/core/optimizer/__init__.py:18: UserWarning: Transformer Engine and Apex are not installed. Falling back to Torch optimizers.\n",
      "mport_ckpt/0       warnings.warn(\n",
      "mport_ckpt/0     \n",
      "mport_ckpt/0 [NeMo W 2025-05-29 21:29:48 nemo_logging:361] /Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/megatron/core/optimizer/optimizer.py:28: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier and multi_tensor_scale\n",
      "mport_ckpt/0       warnings.warn(\n",
      "mport_ckpt/0     \n",
      "mport_ckpt/0 [NeMo W 2025-05-29 21:29:48 nemo_logging:361] /Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/megatron/core/optimizer/clip_grads.py:29: UserWarning: Transformer Engine and Apex are not installed. Falling back to local implementations of multi_tensor_applier, multi_tensor_l2norm, and multi_tensor_scale\n",
      "mport_ckpt/0       warnings.warn(\n",
      "mport_ckpt/0     \n",
      "mport_ckpt/0 [NeMo W 2025-05-29 21:29:48 nemo_logging:361] /Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/megatron/core/transformer/multi_token_prediction.py:60: UserWarning: Apex is not installed. Falling back to Torch Norm\n",
      "mport_ckpt/0       warnings.warn('Apex is not installed. Falling back to Torch Norm')\n",
      "mport_ckpt/0     \n",
      "mport_ckpt/0 [NeMo W 2025-05-29 21:29:48 nemo_logging:361] /Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/megatron/core/models/gpt/gpt_layer_specs.py:65: UserWarning: Apex is not installed. Falling back to Torch Norm\n",
      "mport_ckpt/0       warnings.warn('Apex is not installed. Falling back to Torch Norm')\n",
      "mport_ckpt/0     \n",
      "mport_ckpt/0 [NeMo W 2025-05-29 21:29:48 nemo_logging:361] /Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/megatron/core/models/retro/encoder_spec.py:47: UserWarning: Apex is not installed. Falling back to Torch Norm\n",
      "mport_ckpt/0       warnings.warn(f'Apex is not installed. Falling back to Torch Norm')\n",
      "mport_ckpt/0     \n",
      "mport_ckpt/0 [NeMo W 2025-05-29 21:29:48 nemo_logging:361] /Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/megatron/core/models/retro/decoder_spec.py:39: UserWarning: Apex is not installed. Falling back to Torch Norm\n",
      "mport_ckpt/0       warnings.warn(f'Apex is not installed. Falling back to Torch Norm')\n",
      "mport_ckpt/0     \n",
      "mport_ckpt/0 [NeMo W 2025-05-29 21:29:48 nemo_logging:361] /Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/megatron/core/models/T5/t5_spec.py:43: UserWarning: Apex is not installed. Falling back to Torch Norm\n",
      "mport_ckpt/0       warnings.warn(f'Apex is not installed. Falling back to Torch Norm')\n",
      "mport_ckpt/0     \n",
      "mport_ckpt/0 [NeMo W 2025-05-29 21:29:51 nemo_logging:361] /Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/megatron/core/models/bert/bert_lm_head.py:15: UserWarning: Apex is not installed. Falling back to Torch Norm\n",
      "mport_ckpt/0       warnings.warn(f'Apex is not installed. Falling back to Torch Norm')\n",
      "mport_ckpt/0     \n",
      "mport_ckpt/0 [NeMo W 2025-05-29 21:29:51 ssm:31] The package `megatron.core` was not imported in this environment which is needed for SSMs.\n",
      "mport_ckpt/0 Traceback (most recent call last):\n",
      "mport_ckpt/0   File \"/Users/erlebach/.local/share/uv/python/cpython-3.10.16-macos-aarch64-none/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "mport_ckpt/0     return _run_code(code, main_globals, None,\n",
      "mport_ckpt/0   File \"/Users/erlebach/.local/share/uv/python/cpython-3.10.16-macos-aarch64-none/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "mport_ckpt/0     exec(code, run_globals)\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/nemo_run/core/runners/fdl_runner.py\", line 72, in <module>\n",
      "mport_ckpt/0     fdl_runner_app()\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/typer/main.py\", line 340, in __call__\n",
      "mport_ckpt/0     raise e\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/typer/main.py\", line 323, in __call__\n",
      "mport_ckpt/0     return get_command(self)(*args, **kwargs)\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/click/core.py\", line 1442, in __call__\n",
      "mport_ckpt/0     return self.main(*args, **kwargs)\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/typer/core.py\", line 677, in main\n",
      "mport_ckpt/0     return _main(\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/typer/core.py\", line 195, in _main\n",
      "mport_ckpt/0     rv = self.invoke(ctx)\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/click/core.py\", line 1226, in invoke\n",
      "mport_ckpt/0     return ctx.invoke(self.callback, **ctx.params)\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/click/core.py\", line 794, in invoke\n",
      "mport_ckpt/0     return callback(*args, **kwargs)\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/typer/main.py\", line 698, in wrapper\n",
      "mport_ckpt/0     return callback(**use_params)\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/nemo_run/core/runners/fdl_runner.py\", line 67, in fdl_direct_run\n",
      "mport_ckpt/0     fdl_fn = fdl.build(fdl_buildable)\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/fiddle/_src/building.py\", line 185, in build\n",
      "mport_ckpt/0     result = daglish.MemoizedTraversal.run(_build, buildable)\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/fiddle/_src/daglish.py\", line 477, in run\n",
      "mport_ckpt/0     return fn(root_obj, state)\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/fiddle/_src/building.py\", line 176, in _build\n",
      "mport_ckpt/0     sub_traversal = state.flattened_map_children(value)\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/fiddle/_src/daglish.py\", line 631, in flattened_map_children\n",
      "mport_ckpt/0     return self._flattened_map_children(value, node_traverser)\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/fiddle/_src/daglish.py\", line 579, in _flattened_map_children\n",
      "mport_ckpt/0     new_subvalues = [\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/fiddle/_src/daglish.py\", line 580, in <listcomp>\n",
      "mport_ckpt/0     self.call(subvalue, path_element)\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/fiddle/_src/daglish.py\", line 694, in call\n",
      "mport_ckpt/0     return self.traversal.apply(value, new_state)\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/fiddle/_src/daglish.py\", line 786, in apply\n",
      "mport_ckpt/0     result = self.traversal_fn(value, state)\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/fiddle/_src/building.py\", line 176, in _build\n",
      "mport_ckpt/0     sub_traversal = state.flattened_map_children(value)\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/fiddle/_src/daglish.py\", line 631, in flattened_map_children\n",
      "mport_ckpt/0     return self._flattened_map_children(value, node_traverser)\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/fiddle/_src/daglish.py\", line 579, in _flattened_map_children\n",
      "mport_ckpt/0     new_subvalues = [\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/fiddle/_src/daglish.py\", line 580, in <listcomp>\n",
      "mport_ckpt/0     self.call(subvalue, path_element)\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/fiddle/_src/daglish.py\", line 694, in call\n",
      "mport_ckpt/0     return self.traversal.apply(value, new_state)\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/fiddle/_src/daglish.py\", line 786, in apply\n",
      "mport_ckpt/0     result = self.traversal_fn(value, state)\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/fiddle/_src/building.py\", line 180, in _build\n",
      "mport_ckpt/0     return call_buildable(value, arguments, current_path=state.current_path)\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/fiddle/_src/building.py\", line 119, in call_buildable\n",
      "mport_ckpt/0     with reraised_exception.try_with_lazy_message(make_message):\n",
      "mport_ckpt/0   File \"/Users/erlebach/.local/share/uv/python/cpython-3.10.16-macos-aarch64-none/lib/python3.10/contextlib.py\", line 153, in __exit__\n",
      "mport_ckpt/0     self.gen.throw(typ, value, traceback)\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/fiddle/_src/reraised_exception.py\", line 82, in try_with_lazy_message\n",
      "mport_ckpt/0     raise decorate_exception(exc, message) from None\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/fiddle/_src/reraised_exception.py\", line 74, in try_with_lazy_message\n",
      "mport_ckpt/0     yield\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/fiddle/_src/building.py\", line 120, in call_buildable\n",
      "mport_ckpt/0     return buildable.__build__(*args, **kwargs)\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/fiddle/_src/config.py\", line 783, in __build__\n",
      "mport_ckpt/0     return self.__fn_or_cls__(*args, **kwargs)\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/nemo/lightning/io/mixin.py\", line 579, in wrapped_init\n",
      "mport_ckpt/0     original_init(self, *args, **kwargs)\n",
      "mport_ckpt/0   File \"<string>\", line 190, in __init__\n",
      "mport_ckpt/0   File \"/Users/erlebach/src/2025/gordon_nemo/.venv/lib/python3.10/site-packages/megatron/core/transformer/transformer_config.py\", line 921, in __post_init__\n",
      "mport_ckpt/0     raise ValueError(\n",
      "mport_ckpt/0 ValueError: apply_rope_fusion is not available. Please install TE >= 1.4 or Apex.\n",
      "mport_ckpt/0 \n",
      "mport_ckpt/0 Fiddle context: failed to construct or call Llama3Config8B at <root>.model.config with positional arguments: (), keyword arguments: ().\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job nemo.collections.llm.api.import_ckpt-m661959kz100g finished: FAILED\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># The experiment was run with the following tasks: ['nemo.collections.llm.api.import_ckpt']</span><span style=\"background-color: #272822\">                        </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect and reconstruct this experiment at a later point in time using:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> run</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">Experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">from_id(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.import_ckpt_1748568583\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">)</span><span style=\"background-color: #272822\">                             </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">status() </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the overall status</span><span style=\"background-color: #272822\">                                                                      </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">logs(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.import_ckpt\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Gets the log for the provided task</span><span style=\"background-color: #272822\">                       </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">experiment</span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">.</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">cancel(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"nemo.collections.llm.api.import_ckpt\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">) </span><span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># Cancels the provided task if still running</span><span style=\"background-color: #272822\">             </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# The experiment was run with the following tasks: ['nemo.collections.llm.api.import_ckpt']\u001b[0m\u001b[48;2;39;40;34m                        \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect and reconstruct this experiment at a later point in time using:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mrun\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mExperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mfrom_id\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.import_ckpt_1748568583\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                             \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the overall status\u001b[0m\u001b[48;2;39;40;34m                                                                      \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.import_ckpt\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Gets the log for the provided task\u001b[0m\u001b[48;2;39;40;34m                       \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m.\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mnemo.collections.llm.api.import_ckpt\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;149;144;119;48;2;39;40;34m# Cancels the provided task if still running\u001b[0m\u001b[48;2;39;40;34m             \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "<span style=\"color: #959077; text-decoration-color: #959077; background-color: #272822\"># You can inspect this experiment at a later point in time using the CLI as well:</span><span style=\"background-color: #272822\">                                  </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment status nemo.collections.llm.api.import_ckpt_1748568583</span><span style=\"background-color: #272822\">                                             </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment logs nemo.collections.llm.api.import_ckpt_1748568583 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                             </span>\n",
       "<span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">nemo experiment cancel nemo.collections.llm.api.import_ckpt_1748568583 </span><span style=\"color: #ae81ff; text-decoration-color: #ae81ff; background-color: #272822\">0</span><span style=\"background-color: #272822\">                                           </span>\n",
       "<span style=\"background-color: #272822\">                                                                                                                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n",
       "\u001b[38;2;149;144;119;48;2;39;40;34m# You can inspect this experiment at a later point in time using the CLI as well:\u001b[0m\u001b[48;2;39;40;34m                                  \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mstatus\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.import_ckpt_1748568583\u001b[0m\u001b[48;2;39;40;34m                                             \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mlogs\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.import_ckpt_1748568583\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                             \u001b[0m\n",
       "\u001b[38;2;248;248;242;48;2;39;40;34mnemo\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mexperiment\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mcancel\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mnemo.collections.llm.api.import_ckpt_1748568583\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;174;129;255;48;2;39;40;34m0\u001b[0m\u001b[48;2;39;40;34m                                           \u001b[0m\n",
       "\u001b[48;2;39;40;34m                                                                                                                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nemo_run as run\n",
    "from nemo import lightning as nl\n",
    "from nemo.collections import llm\n",
    "from megatron.core.optimizer import OptimizerConfig\n",
    "import torch\n",
    "import lightning.pytorch as pl\n",
    "from pathlib import Path\n",
    "from nemo.collections.llm.recipes.precision.mixed_precision import bf16_mixed\n",
    "\n",
    "\n",
    "# llm.import_ckpt is the nemo2 API for converting Hugging Face checkpoint to NeMo format\n",
    "# example python usage:\n",
    "# llm.import_ckpt(model=llm.llama3_8b.model(), source=\"hf://meta-llama/Meta-Llama-3-8B\")\n",
    "#\n",
    "# We use run.Partial to configure this function\n",
    "def configure_checkpoint_conversion():\n",
    "    return run.Partial(\n",
    "        llm.import_ckpt,\n",
    "        model=llm.llama3_8b.model(),\n",
    "        source=\"hf://meta-llama/Meta-Llama-3-8B\",\n",
    "        overwrite=False,\n",
    "    )\n",
    "\n",
    "# configure your function\n",
    "import_ckpt = configure_checkpoint_conversion()\n",
    "# define your executor\n",
    "local_executor = run.LocalExecutor()\n",
    "\n",
    "# run your experiment\n",
    "run.run(import_ckpt, executor=local_executor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Prepare the Data and Customize the DataModule\n",
    "\n",
    "We will be using Databricks-dolly-15k for this notebook. NeMo 2.0 already provides a `DollyDataModule`. For all data modules that are included in NeMo 2.0, refer to the [data module directory](https://github.com/NVIDIA/NeMo/tree/main/nemo/collections/llm/gpt/data). Example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dolly() -> run.Config[pl.LightningDataModule]:\n",
    "    return run.Config(llm.DollyDataModule, seq_length=2048, micro_batch_size=1, global_batch_size=8, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use your own data, you will need to create a custom `DataModule`. This involves extending the base class `FineTuningDataModule` so that you have access to existing data handling logic, such as packed sequences. Here we walk you through the process step by step, using the already existing [`DollyDataModule`](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/llm/gpt/data/dolly.py) as an example. \n",
    "\n",
    "### Subclass the FineTuningDataModule\n",
    "You need to extend the `FineTuningDataModule` if you're fine-tuning NeMo models. This provides access to existing data handling logic, such as packed sequences. The `data_root` parameter is where you store your generated `train/validation/test.jsonl` in NeMo format. Below is how `DollyDataModule` does it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from nemo.lightning.io.mixin import IOMixin\n",
    "from nemo.collections.llm.gpt.data.fine_tuning import FineTuningDataModule\n",
    "\n",
    "class DollyDataModule(FineTuningDataModule, IOMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        seq_length: int = 2048,\n",
    "        tokenizer: Optional[\"TokenizerSpec\"] = None,\n",
    "        micro_batch_size: int = 4,\n",
    "        global_batch_size: int = 8,\n",
    "        rampup_batch_size: Optional[List[int]] = None,\n",
    "        force_redownload: bool = False,\n",
    "        delete_raw: bool = True,\n",
    "        seed: int = 1234,\n",
    "        memmap_workers: int = 1,\n",
    "        num_workers: int = 8,\n",
    "        pin_memory: bool = True,\n",
    "        persistent_workers: bool = False,\n",
    "        pad_to_max_length: bool = False,\n",
    "        packed_sequence_size: int = -1,\n",
    "    ):\n",
    "        self.force_redownload = force_redownload\n",
    "        self.delete_raw = delete_raw\n",
    "\n",
    "        super().__init__(\n",
    "            dataset_root=get_dataset_root(\"dolly\"),\n",
    "            seq_length=seq_length,\n",
    "            tokenizer=tokenizer,\n",
    "            micro_batch_size=micro_batch_size,\n",
    "            global_batch_size=global_batch_size,\n",
    "            rampup_batch_size=rampup_batch_size,\n",
    "            seed=seed,\n",
    "            memmap_workers=memmap_workers,\n",
    "            num_workers=num_workers,\n",
    "            pin_memory=pin_memory,\n",
    "            persistent_workers=persistent_workers,\n",
    "            pad_to_max_length=pad_to_max_length,\n",
    "            packed_sequence_size=packed_sequence_size,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Override the `prepare_data` Method\n",
    "\n",
    "The `prepare_data` method is responsible for downloading and preprocessing data if needed. If the dataset is already downloaded, you can skip this step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_data(self) -> None:\n",
    "    # if train file is specified, no need to do anything\n",
    "    if not self.train_path.exists() or self.force_redownload:\n",
    "        dset = self._download_data()\n",
    "        self._preprocess_and_split_data(dset)\n",
    "    super().prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Data Download and Preprocessing Logic\n",
    "\n",
    "If your dataset requires downloading or preprocessing, implement this logic within the helper methods. Skip the download part if it's not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _download_data(self):\n",
    "    logging.info(f\"Downloading {self.__class__.__name__}...\")\n",
    "    return load_dataset(\n",
    "        \"databricks/databricks-dolly-15k\",\n",
    "        cache_dir=str(self.dataset_root),\n",
    "        download_mode=\"force_redownload\" if self.force_redownload else None,\n",
    "    )\n",
    "\n",
    "def _preprocess_and_split_data(self, dset, train_ratio: float = 0.80, val_ratio: float = 0.15):\n",
    "    logging.info(f\"Preprocessing {self.__class__.__name__} to jsonl format and splitting...\")\n",
    "\n",
    "    test_ratio = 1 - train_ratio - val_ratio\n",
    "    save_splits = {}\n",
    "    dataset = dset.get('train')\n",
    "    split_dataset = dataset.train_test_split(test_size=val_ratio + test_ratio, seed=self.seed)\n",
    "    split_dataset2 = split_dataset['test'].train_test_split(\n",
    "        test_size=test_ratio / (val_ratio + test_ratio), seed=self.seed\n",
    "    )\n",
    "    save_splits['training'] = split_dataset['train']\n",
    "    save_splits['validation'] = split_dataset2['train']\n",
    "    save_splits['test'] = split_dataset2['test']\n",
    "\n",
    "    for split_name, dataset in save_splits.items():\n",
    "        output_file = self.dataset_root / f\"{split_name}.jsonl\"\n",
    "        with output_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            for example in dataset:\n",
    "                context = example[\"context\"].strip()\n",
    "                if context != \"\":\n",
    "                    # Randomize context and instruction order.\n",
    "                    context_first = np.random.randint(0, 2) == 0\n",
    "                    if context_first:\n",
    "                        instruction = example[\"instruction\"].strip()\n",
    "                        assert instruction != \"\"\n",
    "                        _input = f\"{context}\\n\\n{instruction}\"\n",
    "                        _output = example[\"response\"]\n",
    "                    else:\n",
    "                        instruction = example[\"instruction\"].strip()\n",
    "                        assert instruction != \"\"\n",
    "                        _input = f\"{instruction}\\n\\n{context}\"\n",
    "                        _output = example[\"response\"]\n",
    "                else:\n",
    "                    _input = example[\"instruction\"]\n",
    "                    _output = example[\"response\"]\n",
    "\n",
    "                f.write(json.dumps({\"input\": _input, \"output\": _output, \"category\": example[\"category\"]}) + \"\\n\")\n",
    "\n",
    "        logging.info(f\"{split_name} split saved to {output_file}\")\n",
    "\n",
    "    if self.delete_raw:\n",
    "        for p in self.dataset_root.iterdir():\n",
    "            if p.is_dir():\n",
    "                shutil.rmtree(p)\n",
    "            elif '.jsonl' not in str(p.name):\n",
    "                p.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original example in Dolly dataset looks like:\n",
    "```\n",
    "{'instruction': 'Extract all the movies from this passage and the year they were released out. Write each movie as a separate sentence', 'context': \"The genre has existed since the early years of silent cinema, when Georges Melies' A Trip to the Moon (1902) employed trick photography effects. The next major example (first in feature length in the genre) was the film Metropolis (1927). From the 1930s to the 1950s, the genre consisted mainly of low-budget B movies. After Stanley Kubrick's landmark 2001: A Space Odyssey (1968), the science fiction film genre was taken more seriously. In the late 1970s, big-budget science fiction films filled with special effects became popular with audiences after the success of Star Wars (1977) and paved the way for the blockbuster hits of subsequent decades.\", 'response': 'A Trip to the Moon was released in 1902. Metropolis came out in 1927. 2001: A Space Odyssey was released in 1968. Star Wars came out in 1977.', 'category': 'information_extraction'}\n",
    "```\n",
    "After the preprocessing logic, the data examples are transformed into NeMo format, as below:\n",
    "```\n",
    "{'input': \"Extract all the movies from this passage and the year they were released out. Write each movie as a separate sentence\\n\\nThe genre has existed since the early years of silent cinema, when Georges Melies' A Trip to the Moon (1902) employed trick photography effects. The next major example (first in feature length in the genre) was the film Metropolis (1927). From the 1930s to the 1950s, the genre consisted mainly of low-budget B movies. After Stanley Kubrick's landmark 2001: A Space Odyssey (1968), the science fiction film genre was taken more seriously. In the late 1970s, big-budget science fiction films filled with special effects became popular with audiences after the success of Star Wars (1977) and paved the way for the blockbuster hits of subsequent decades.\", 'output': 'A Trip to the Moon was released in 1902. Metropolis came out in 1927. 2001: A Space Odyssey was released in 1968. Star Wars came out in 1977.', 'category': 'information_extraction'}\n",
    "```\n",
    "Each data example is saved as a json string as one line in the `train/validation/test.jsonl` file, under `data_root` directory you specified earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.1: Configure SFT with the NeMo 2.0 API \n",
    "\n",
    "In this notebook we use NeMo 2.0 API to perform SFT. First we configure the following components for training. These components are similar between SFT and PEFT. SFT and PEFT both uses `llm.finetune` API. To switch from PEFT to SFT, you just need to remove the `peft` parameter.\n",
    "\n",
    "### Configure the Trainer\n",
    "The NeMo 2.0 Trainer works similarly to the PyTorch Lightning trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainer() -> run.Config[nl.Trainer]:\n",
    "    strategy = run.Config(\n",
    "        nl.MegatronStrategy,\n",
    "        tensor_model_parallel_size=2\n",
    "    )\n",
    "    trainer = run.Config(\n",
    "        nl.Trainer,\n",
    "        devices=2,\n",
    "        max_steps=20,\n",
    "        accelerator=\"gpu\",\n",
    "        strategy=strategy,\n",
    "        plugins=bf16_mixed(),\n",
    "        log_every_n_steps=1,\n",
    "        limit_val_batches=2,\n",
    "        val_check_interval=2,\n",
    "        num_sanity_val_steps=0,\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Configure the Logger\n",
    "Configure your training steps, output directories and logging through `NeMoLogger`. In the following example, the experiment output will be saved at `./results/nemo2_sft`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def logger() -> run.Config[nl.NeMoLogger]:\n",
    "    ckpt = run.Config(\n",
    "        nl.ModelCheckpoint,\n",
    "        save_last=True,\n",
    "        every_n_train_steps=10,\n",
    "        monitor=\"reduced_train_loss\",\n",
    "        save_top_k=1,\n",
    "        save_on_train_epoch_end=True,\n",
    "        save_optim_on_train_end=True,\n",
    "    )\n",
    "\n",
    "    return run.Config(\n",
    "        nl.NeMoLogger,\n",
    "        name=\"nemo2_sft\",\n",
    "        log_dir=\"./results\",\n",
    "        use_datetime_version=False,\n",
    "        ckpt=ckpt,\n",
    "        wandb=None\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Configure the Optimizer\n",
    "In the following example, we will be using the distributed adam optimizer and pass in the optimizer configuration through `OptimizerConfig`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def adam_with_cosine_annealing() -> run.Config[nl.OptimizerModule]:\n",
    "    opt_cfg = run.Config(\n",
    "        OptimizerConfig,\n",
    "        optimizer=\"adam\",\n",
    "        lr=5e-6,\n",
    "        adam_beta2=0.98,\n",
    "        use_distributed_optimizer=True,\n",
    "        clip_grad=1.0,\n",
    "        bf16=True,\n",
    "    )\n",
    "    return run.Config(\n",
    "        nl.MegatronOptimizerModule,\n",
    "        config=opt_cfg\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the Base Model\n",
    "We will perform SFT on top of Llama3-8B, so we create a `LlamaModel` to pass to the finetune API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def llama3_8b() -> run.Config[pl.LightningModule]:\n",
    "    return run.Config(llm.LlamaModel, config=run.Config(llm.Llama3Config8B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Resume\n",
    "In NeMo 2.0, we can directly pass in the Llama3-8b Hugging Face ID to start SFT without manually converting it into the NeMo checkpoint format, as required in NeMo 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def resume() -> run.Config[nl.AutoResume]:\n",
    "    return run.Config(\n",
    "        nl.AutoResume,\n",
    "        restore_config=run.Config(nl.RestoreConfig,\n",
    "            path=\"nemo://meta-llama/Meta-Llama-3-8B\"\n",
    "        ),\n",
    "        resume_if_exists=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Configure NeMo 2.0 finetune API\n",
    "Using all the components we created above, we can call the NeMo 2.0 finetune API. The python example usage is as below:\n",
    "```\n",
    "llm.finetune(\n",
    "    model=llama3_8b(),\n",
    "    data=dolly(),\n",
    "    trainer=trainer(),\n",
    "    log=logger(),\n",
    "    optim=adam_with_cosine_annealing(),\n",
    "    resume=resume(),\n",
    ")\n",
    "```\n",
    "We configure the `llm.finetune` API as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_finetuning_recipe():\n",
    "    return run.Partial(\n",
    "        llm.finetune,\n",
    "        model=llama3_8b(),\n",
    "        trainer=trainer(),\n",
    "        data=dolly(),\n",
    "        log=logger(),\n",
    "        optim=adam_with_cosine_annealing(),\n",
    "        resume=resume(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.2: Run SFT with NeMo 2.0 API and NeMo-Run\n",
    "\n",
    "We use `LocalExecutor` for executing our configured finetune function. For more details on the NeMo-Run executor, refer to [Execute NeMo Run](https://github.com/NVIDIA/NeMo-Run/blob/main/docs/source/guides/execution.md) of NeMo-Run Guides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def local_executor_torchrun(nodes: int = 1, devices: int = 2) -> run.LocalExecutor:\n",
    "    # Env vars for jobs are configured here\n",
    "    env_vars = {\n",
    "        \"TORCH_NCCL_AVOID_RECORD_STREAMS\": \"1\",\n",
    "        \"NCCL_NVLS_ENABLE\": \"0\",\n",
    "    }\n",
    "\n",
    "    executor = run.LocalExecutor(ntasks_per_node=devices, launcher=\"torchrun\", env_vars=env_vars)\n",
    "\n",
    "    return executor\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run.run(configure_finetuning_recipe(), executor=local_executor_torchrun())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Generate Results from Trained SFT Checkpoints\n",
    "\n",
    "We use the `llm.generate` API in NeMo 2.0 to generate results from the trained SFT checkpoint. Find your last saved checkpoint from your experiment dir: `results/nemo2_sft/checkpoints`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sft_ckpt_path=str(next((d for d in Path(\"./results/nemo2_sft/checkpoints/\").iterdir() if d.is_dir() and d.name.endswith(\"-last\")), None))\n",
    "print(\"We will load SFT checkpoint from:\", sft_ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using `llm.generate` API, you can pass a data module such as dolly: `input_dataset=dolly()`. This will use the test set from the specified data module to generate predictions. In the following example, the generated predictions are saved to the `sft_predictions.txt` file. Note that while fine-tuning required a minimum of 2 GPUs with `tensor_model_parallel_size=2`, generating predictions only requires `tensor_model_parallel_size=1`. However, using multiple GPUs can speed up the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from megatron.core.inference.common_inference_params import CommonInferenceParams\n",
    "\n",
    "\n",
    "def trainer() -> run.Config[nl.Trainer]:\n",
    "    strategy = run.Config(\n",
    "        nl.MegatronStrategy,\n",
    "        tensor_model_parallel_size=1,\n",
    "    )\n",
    "    trainer = run.Config(\n",
    "        nl.Trainer,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1,\n",
    "        num_nodes=1,\n",
    "        strategy=strategy,\n",
    "        plugins=bf16_mixed(),\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "def configure_inference():\n",
    "    return run.Partial(\n",
    "        llm.generate,\n",
    "        path=str(sft_ckpt_path),\n",
    "        trainer=trainer(),\n",
    "        input_dataset=dolly(),\n",
    "        inference_params=CommonInferenceParams(num_tokens_to_generate=20, top_k=1),\n",
    "        output_path=\"sft_prediction.jsonl\",\n",
    "    )\n",
    "\n",
    "\n",
    "def local_executor_torchrun(nodes: int = 1, devices: int = 1) -> run.LocalExecutor:\n",
    "    # Env vars for jobs are configured here\n",
    "    env_vars = {\n",
    "        \"TORCH_NCCL_AVOID_RECORD_STREAMS\": \"1\",\n",
    "        \"NCCL_NVLS_ENABLE\": \"0\",\n",
    "    }\n",
    "\n",
    "    executor = run.LocalExecutor(ntasks_per_node=devices, launcher=\"torchrun\", env_vars=env_vars)\n",
    "\n",
    "    return executor\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run.run(configure_inference(), executor=local_executor_torchrun())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the inference is complete, you will see results similar to the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "head -n 3 sft_prediction.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see output similar to the following:\n",
    "```\n",
    "{\"input\": \"What is best creator's platform\", \"category\": \"brainstorming\", \"label\": \"Youtube. Youtube should be best creator platform\", \"prediction\": \" for video content creators. YouTube is best creator's platform for video content creators.\"}\n",
    "{\"input\": \"When was the last time the Raiders won the Super Bowl?\", \"category\": \"open_qa\", \"label\": \"The Raiders have won three Super Bowl championships (1977, 1981, and 1984), one American Football League (AFL) championship (1967), and four American Football Conference (AFC) titles. The most recent Super Bowl ring was won in 1984 against the Washington Redskins of the NFC.\", \"prediction\": \" 2003\"}\n",
    "{\"input\": \"Muckle Water is a long, narrow fresh water loch on Ward Hill on Rousay, Orkney, Scotland. It is the biggest loch on the island and is popular for fishing. It can be reached by a track from the roadside. The Suso Burn on the north eastern shore drains the loch into the Sound of Rousay.\\n\\nWhere is Muckle Water?\", \"category\": \"closed_qa\", \"label\": \"Muckle water is located in Rousay, Orkney, Scotland.\", \"prediction\": \" Muckle Water is a long, narrow fresh water loch on Ward Hill on Rousay,\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Calculate Evaluation Metrics\n",
    "\n",
    "We can evaluate the model's predictions by calculating the Exact Match (EM) and F1 scores.\n",
    "- Exact Match is a binary measure (0 or 1) checking if the model outputs match one of the\n",
    "ground truth answer exactly.\n",
    "- F1 score is the harmonic mean of precision and recall for the answer words.\n",
    "\n",
    "Below is a script that computes these metrics. The sample scores can be improved by training the model further and performing hyperparameter tuning. In this notebook, we only train for 20 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python /opt/NeMo/scripts/metric_calculation/peft_metric_calc.py --pred_file sft_prediction.jsonl --label_field \"label\" --pred_field \"prediction\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
